LLM: ChatGPT-4.
Date: November 4 2024

You said: 
evaluate the csv attached and answer: 
what is the population, frame, and sample;
how is the sample recruited;
what sampling approach is taken, and what are some of the trade-offs of this;
how is non-response handled;
what is good and bad about the questionnaire.

ChatGPT:
Pollster Overview
Trafalgar Group is a polling organization known for conducting political polls, including those relevant to U.S. elections. The data shows a pollster rating of 0.7, indicating its overall accuracy and reliability compared to other pollsters.
Methodology
Population, Frame, and Sample
Population: Trafalgar Group focuses on the voting population of the United States during political events such as elections. This includes individuals eligible to vote.
Sampling Frame: The frame would likely consist of registered voters across states, often segmented by demographics like age, gender, and party affiliation.
Sample: The sample for the polls typically includes likely voters. Trafalgar Group tends to focus on selecting individuals who are most likely to participate in upcoming elections.


Interactive voice response/Live Phone/Text/Online Panel/Email
Sample Recruitment
Trafalgar Group often employs a combination of methods, including live telephone interviews, automated voice responses, and online panels. This multi-modal approach helps capture a broader audience.
Advantages: This increases inclusivity, ensuring different groups are represented in the sample.
Trade-offs: While this mixed methodology can enhance representation, it might introduce biases if certain segments, like older voters, respond more to phone surveys, and younger individuals are underrepresented.
Sampling Approach
Trafalgar Group uses stratified random sampling, which ensures that subgroups (e.g., political party, gender, region) are represented proportionately in the sample.
Strengths: Stratified sampling can improve accuracy by ensuring the sample mirrors the actual electorate's demographic and political composition.
Weaknesses: This approach might lead to over-stratification, where smaller or more niche voter groups receive disproportionate weight, potentially skewing results.
Non-response Handling
Trafalgar Group incorporates weighting to correct for non-response. After data collection, they apply post-survey adjustments to balance the demographic characteristics of respondents with known population parameters (age, race, education).
Strengths: This method helps mitigate non-response bias by ensuring that underrepresented groups are accounted for in the final results.
Weaknesses: Heavy reliance on post-survey weighting can sometimes introduce its own biases, especially if there are large gaps in response rates among certain demographics.
Questionnaire Design
Trafalgar Group's questionnaires are typically short, focused on candidate preference, and key political issues. Questions tend to be direct to avoid confusion and ensure high response rates.
Strengths: The simplicity of the questionnaire reduces respondent fatigue, making it easier to gather accurate information.
Weaknesses: Limited questionnaire depth can sometimes lead to missing nuances, such as voter indecision or deeper political opinions.
Key Features
Hybrid Methodology: Trafalgar Group's hybrid approach of using live interviews, automated calls, and online surveys broadens access to different demographics but also presents challenges in balancing the modes effectively.
Focus on Likely Voters: By concentrating on likely voters, Trafalgar Group hones in on the population segment that will most influence election outcomes. However, this focus can sometimes miss trends among non-traditional or undecided voters.
Strengths and Weaknesses Summary
Strengths:
Efficient Sampling: Use of stratified sampling ensures a representative sample from various political and demographic groups.
Hybrid Data Collection: A multi-modal approach helps capture a diverse set of responses.
Non-response Handling: Weighting corrections help manage the biases from non-response.
Weaknesses:
Post-weighting Issues: Heavy reliance on weighting to correct sample discrepancies can introduce its own inaccuracies.
Over-simplification of Questions: Trafalgar's focus on brevity in questionnaires might overlook voter complexities or ambivalence.
This methodological deep dive reveals that while Trafalgar Group's approach is generally robust, it carries inherent trade-offs in terms of sampling depth and the handling of non-response. The hybrid data collection strategy is key to the group’s methodology, balancing accessibility and response accuracy, but it also requires careful post-survey adjustments to ensure valid results. 

You said: please put together an idealized methodology and survey that you would run if you had a budget of $100K and the task of forecasting the US presidential election. You should detail the sampling approach that you would use, how you would recruit respondents, data validation, and any other relevant aspects of interest. Also be careful to address any poll aggregation or other features of your methodology. You should actually implement your survey using a survey platform like Google Forms.
ChatGPT:
If I had a budget of $100K to forecast the U.S. presidential election, my methodology would combine stratified random sampling and quota sampling to ensure a representative sample. Here's a breakdown:
Sampling Approach:
I would use stratified random sampling to divide the population by key demographics like age, race, gender, and political affiliation, ensuring each subgroup is proportionally represented. Within each stratum, I would apply quota sampling to maintain balance, ensuring I hit targets for underrepresented groups like younger or minority voters.
Recruitment:
Respondents would be recruited through a mix of methods:
Phone Surveys: Both landlines and cellphones, using interactive voice response (IVR) for cost efficiency.
Online Panels: Partnering with reputable platforms to access verified U.S. voters.
Text and Email Surveys: Targeting likely voters through opt-in lists and voter registration databases.
Survey Methodology:
The survey would run across multiple channels (phone, online, text) to ensure comprehensive coverage. I’d aim for a sample size of around 10,000 voters to maintain a margin of error of approximately ±1%. The questionnaire would include demographic questions, voting intention, candidate favorability, and key election issues.
Data Validation:
I’d use data validation techniques to cross-check respondent demographics with official U.S. voter registration data. Post-survey, I'd apply weighting adjustments to correct for non-response bias, balancing the sample to match national population parameters.
Poll Aggregation:
To improve reliability, I’d aggregate results across multiple waves of surveys over time, conducting weekly polls until Election Day. Polling data would be weighted by recency and sample quality to produce a rolling average forecast.
Budget Allocation:
$40K: Phone surveys (live calls + IVR).
$30K: Online panel partnerships and recruitment.
$15K: Data analysis, weighting, and validation tools.
$10K: Software and platform fees (Google Forms, survey software).
$5K: Marketing, incentives for respondents.
This approach ensures both cost-effectiveness and accuracy, with multiple validation steps to produce a reliable forecast of the U.S. presidential election.
4o
You said:
create a survey for me
ChatGPT said:
ChatGPT
Here is an ideal structure for your U.S. presidential election forecasting survey:
Survey Title: U.S. Presidential Election Forecasting Survey
Introduction:
Thank you for participating in our election survey. Your responses will help forecast the 2024 U.S. presidential election. All answers are confidential and anonymous. This survey should take around 5 minutes to complete.
Section 1: Demographics
Age:
Under 18
18-29
30-44
45-60
61 and above
Gender:
Male
Female
Non-binary/Other
Prefer not to say
Race/Ethnicity:
White
Black/African American
Hispanic/Latino
Asian
Native American
Other
Political Affiliation:
Democrat
Republican
Independent
Other
State of Residence: (Dropdown list of U.S. states)
Section 2: Voting Intentions
Are you registered to vote in the 2024 presidential election?
Yes
No
How likely are you to vote in the 2024 presidential election?
Very likely
Somewhat likely
Not sure
Unlikely
Which candidate do you plan to vote for?
Democratic candidate
Republican candidate
Third-party candidate
Undecided
Have you participated in early voting or plan to?
Yes, I have already voted
Yes, I plan to vote early
No, I will vote on Election Day
No, I do not plan to vote
Section 3: Key Issues
What is the most important issue influencing your vote?
Economy
Healthcare
Immigration
Climate Change
Social Justice
National Security
Other
Section 4: Candidate Favorability
Please rate your favorability for the following candidates (1-5 scale):
Democratic candidate
Republican candidate
Other candidates
Submit:
Thank you for your time! Please click submit to record your responses.

You can implement this survey using Google Forms or another survey platform to collect data easily and analyze the results. Let me know if you'd like further customization or help with implementation!

You said:
I have data from various polls, this is an example of header and data (poll_id,pollster_id,pollster,sponsor_ids,sponsors,display_name,pollster_rating_id,pollster_rating_name,numeric_grade,pollscore,methodology,transparency_score,state,start_date,end_date,sponsor_candidate_id,sponsor_candidate,sponsor_candidate_party,endorsed_candidate_id,endorsed_candidate_name,endorsed_candidate_party,question_id,sample_size,population,subpopulation,population_full,tracking,created_at,notes,url,url_article,url_topline,url_crosstab,source,internal,partisan,race_id,cycle,office_type,seat_number,seat_name,election_date,stage,nationwide_batch,ranked_choice_reallocated,ranked_choice_round,hypothetical,party,answer,candidate_id,candidate_name,pct
88781,1697,Bullfinch,"","",The Bullfinch Group,692,The Bullfinch Group,,,Online Panel,4.0,Wisconsin,10/11/24,10/18/24,,"",,,,,213309,600,lv,,lv,,10/18/24 14:49,"",https://www.thebullfinchgroup.com/post/your-election-guide-for-the-midrust-battlegrounds,https://www.thebullfinchgroup.com/post/your-election-guide-for-the-midrust-battlegrounds,https://projects.fivethirtyeight.com/polls/20241018_BlueWall_Bullfinch.pdf,,"",,,8905,2024,U.S. President,0,,11/5/24,general,false,false,,false,DEM,Harris,16661,Kamala Harris,50.0 I want to select unqiue candidate from dataset and group them and calculate average pct of each person
ChatGPT said:
ChatGPT
You can calculate the unique candidates' average percentage support (pct) by grouping them in R. Here's an example of how you can do it:

r
Copy code
# Load necessary library
library(dplyr)

# Assuming your data is stored in a data frame called 'poll_data'
# Select unique candidates and calculate average pct for each candidate
candidate_avg_pct <- poll_data %>%
  group_by(candidate_name) %>%
  summarize(average_pct = mean(pct, na.rm = TRUE))

# View the result
print(candidate_avg_pct)
This code will group the data by candidate_name and then calculate the average pct for each unique candidate.

You said:
should I put pollster as a predictor? there are too many and most have >0.5 as p value which indicates not cignificant
ChatGPT said:
ChatGPT
If many of the pollster variables are showing p-values > 0.5, this suggests they may not be statistically significant predictors for your model. Including too many non-significant predictors can lead to overfitting and make the model less interpretable.

You can consider removing pollster as a predictor or using a strategy like:

Combining pollsters into broader categories (e.g., by methodology or reliability score).
Using a stepwise regression or regularization methods (like Lasso) to reduce the number of predictors automatically.

You said:
how to select top 10 by average_pct from candidate_avg_pct

ChatGPT said:
ChatGPT
To select the top 10 candidates by average_pct in R from your candidate_avg_pct data frame, you can use the arrange() and head() functions from the dplyr package. Here's how:

r
Copy code
# Select the top 10 candidates by average_pct
top_10_candidates <- candidate_avg_pct %>%
  arrange(desc(average_pct)) %>%  # Sort by average_pct in descending order
  head(10)  # Select the top 10 rows

# View the result
print(top_10_candidates)
This code will give you the top 10 candidates based on their average percentage (average_pct).

You said:
end date for poll is in format 8/30/2023, how to filter to keep entries in 2024 
ChatGPT said:
ChatGPT
To filter entries and keep only those from 2024 based on the end_date column in your dataset, you can use the following R code:

r
Copy code
# Ensure the necessary library is loaded
library(dplyr)

# Convert end_date to Date format and filter for entries in 2024
poll_data_filtered <- poll_data %>%
  mutate(end_date = as.Date(end_date, format = "%m/%d/%Y")) %>%
  filter(format(end_date, "%Y") == "2024")

# View the filtered data
print(poll_data_filtered)
This code converts the end_date to a Date object, then filters the dataset to keep only entries where the year is 2024.


You said:
I want to select end_date is in 2024, and state is 'NA' which represent survey across all state
ChatGPT said:
ChatGPT
You can filter the data in R by selecting entries where the end_date is in 2024 and the state is "NA" (representing nationwide polls). Here’s the code:

r
Copy code
# Ensure necessary libraries are loaded
library(dplyr)

# Convert end_date to Date format and filter for entries in 2024 and state is 'NA'
poll_data_filtered <- poll_data %>%
  mutate(end_date = as.Date(end_date, format = "%m/%d/%Y")) %>%
  filter(format(end_date, "%Y") == "2024" & state == "NA")

# View the filtered data
print(poll_data_filtered)
This code filters the dataset for entries in 2024 and where state is 'NA'.

You said:
how to use for loop to loop through distinct parties and generate linear models
ChatGPT said:
ChatGPT
To loop through distinct parties and generate linear models for each, you can use a for loop or apply functions like lapply() in R. Here’s how you can use a for loop to accomplish this task:

Steps:
Extract Unique Parties: First, extract the unique values for the party column.
Loop Through Each Party: Fit a linear model for each unique party.
Store the Results: Store the results (e.g., in a list) for further analysis or comparison.
Here’s how to do it using a for loop:

r
Copy code
# Load necessary libraries
library(dplyr)

# Extract unique parties
unique_parties <- unique(poll_data$party)

# Create an empty list to store models
models <- list()

# Loop through each party and fit the linear model
for (party in unique_parties) {
  # Subset the data for the current party
  party_data <- filter(poll_data, party == !!party)
  
  # Fit the linear model for the current party
  model <- lm(pct ~ log(sample_size) + pollscore + methodology, data = party_data)
  
  # Store the model in the list, using the party name as the key
  models[[party]] <- model
}

# Display the models for each party
models
Explanation:
unique(poll_data$party): Extracts the unique values of the party column.
filter(poll_data, party == !!party): Subsets the data for the current party within the loop.
lm(): Fits the linear model for the current party using predictors like log(sample_size), pollscore, and methodology.
models[[party]] <- model: Stores the fitted model in the models list, where each model is associated with its respective party.
Accessing and Summarizing the Models:
After the loop, you can access each party's model from the list and use summary() to view the details:

r
Copy code
# Example: View the summary of the model for the Democratic party
summary(models[["DEM"]])

# Example: View the summary of the model for the Republican party
summary(models[["REP"]])
Additional Notes:
You can add more predictors to the model based on your data and hypothesis.
This approach is flexible, and you can easily modify it to save the results (e.g., residuals, coefficients) or to perform cross-validation for each model.

You said:
model_date_pollster <- lm(pct ~ end_date + pollster, data = just_harris_high_quality)

# Augment data with model predictions
just_harris_high_quality <- just_harris_high_quality |>
  mutate(
    fitted_date = predict(model_date),
    fitted_date_pollster = predict(model_date_pollster)
  ) this is an example model and what is augment step doing?

ChatGPT said:
ChatGPT
The augment step in your code is adding predicted values from your model into your existing dataset (just_harris_high_quality). Specifically, you are creating two new columns with predicted values:

Detailed Explanation:
fitted_date: This column is being created using predictions from a model that only includes end_date as the predictor (which you called model_date).

r
Copy code
fitted_date = predict(model_date)
This line applies the predict() function to the model_date (a linear model) and adds the predicted percentage support (pct) based only on the end_date for each observation in just_harris_high_quality.
fitted_date_pollster: This column is being created using predictions from the model that includes both end_date and pollster as predictors (your model_date_pollster).

r
Copy code
fitted_date_pollster = predict(model_date_pollster)
This line applies the predict() function to the model_date_pollster model and adds the predicted percentage support based on both end_date and pollster for each observation.
What is Augment Doing?
The mutate() function is adding the predictions to your existing dataset by creating new columns (fitted_date and fitted_date_pollster). Essentially, this step is enhancing (or augmenting) the dataset with fitted values from your models. These fitted values represent the model's predictions for the pct variable, given the predictor variables (end_date or end_date and pollster).

This augmentation is useful for comparing the actual values of pct in your dataset to the predicted values and for further analysis, such as calculating residuals or visualizing how well the model fits the data.






You said:
should I do same thing for my model of different parties?
ChatGPT said:
ChatGPT
Yes, you can apply the same process to your models for different parties. Augmenting your data with predictions from models for each party allows you to compare the model’s predictions with actual values for each party’s data. This step is especially useful for evaluating model fit, generating visualizations (like residual vs. fitted plots), and calculating performance metrics such as residuals.

Why Augment for Each Party Model?
Comparing Predictions: Augmenting the data with predictions from different models allows you to compare how well each party's model fits the data. You can see how predicted values match up against actual percentages of support (pct).

Calculating Residuals: By adding the predicted values, you can easily calculate residuals (actual pct minus predicted pct), which helps diagnose model performance.

Visualization: Augmented data makes it easier to plot actual vs. predicted values and evaluate model performance visually. For example, you could plot fitted values for each party’s model on the same graph to compare party-based trends.

Example Code to Apply to Different Party Models:
Let’s assume you have separate models for Democratic and Republican party data:

r
Copy code
# Fit models for each party
model_dem <- lm(pct ~ log(sample_size) + pollscore + numeric_grade + transparency_score + days_since_start, data = dem_data)
model_rep <- lm(pct ~ log(sample_size) + pollscore + numeric_grade + transparency_score + days_since_start, data = rep_data)

# Augment the datasets with predictions
dem_data <- dem_data |>
  mutate(fitted_pct_dem = predict(model_dem))

rep_data <- rep_data |>
  mutate(fitted_pct_rep = predict(model_rep))

# View augmented data
head(dem_data)
head(rep_data)
Steps:
Fit separate models for each party (e.g., Democratic and Republican).
Augment each party’s dataset with predicted values (fitted_pct_dem, fitted_pct_rep) based on their respective models.
Use these augmented datasets to compare model performance and examine differences between how the models for different parties behave.
Benefits:
This method ensures you can evaluate and compare the performance of each model across parties.
It makes it easier to generate plots such as actual vs. predicted or residuals vs. fitted for each party.
By augmenting each party's data with predicted values, you'll be able to better understand how well each model explains voter support and identify any potential issues in party-specific predictions.

You said:
why I get Error in mutate():
ℹ In argument: fitted_pct_dem = predict(models[["DEM"]]).
Caused by error:
! fitted_pct_dem must be size 1245 or 1, not 1235.
ChatGPT said:
ChatGPT
The error you're seeing indicates a size mismatch between the number of observations in your dataset (dem_data with 1245 rows) and the number of predicted values from predict(models[["DEM"]]) (which is returning 1235 values).

Common Causes of This Error:
Missing Data: There might be missing (NA) values in the dataset that the model doesn't handle, resulting in fewer rows being used for prediction than are present in the original dataset.
Filtered Data: If you applied filters to the data before fitting the model but are using the unfiltered data when augmenting it, the number of rows may differ.
How to Fix It:
You need to ensure that the predict() function generates predictions for all rows in the dataset. Here are a few solutions:

Solution 1: Check for Missing Values
Ensure that the dataset used for the model doesn't have missing values in any of the predictors. Missing values in predictors will cause some rows to be excluded from the prediction process.

r
Copy code
# Check for missing values in the dataset
summary(dem_data)

# Optionally, remove rows with missing values
dem_data_clean <- na.omit(dem_data)
Solution 2: Use the Same Data for Prediction
If you've filtered or modified the data before fitting the model, ensure you're using the same data for prediction. For instance, if you dropped rows with NA during model fitting, do the same before augmenting the dataset.

r
Copy code
# Use the same filtered dataset for prediction
dem_data_clean <- dem_data %>%
  filter(!is.na(sample_size), !is.na(pollscore), !is.na(numeric_grade), !is.na(transparency_score), !is.na(days_since_start))

# Fit the model using the cleaned data
model_dem <- lm(pct ~ log(sample_size) + pollscore + numeric_grade + transparency_score + days_since_start, data = dem_data_clean)

# Augment with predictions
dem_data_clean <- dem_data_clean %>%
  mutate(fitted_pct_dem = predict(model_dem))
Solution 3: Handle Missing Predictions
If missing predictions are expected due to missing values, you can assign NA to those predictions to maintain the same row count:

r
Copy code
dem_data <- dem_data %>%
  mutate(fitted_pct_dem = ifelse(!complete.cases(.), NA, predict(model_dem, newdata = dem_data)))
This way, the number of predictions matches the size of the dataset, even when some rows have missing predictor values.

Conclusion:
The key is to ensure that the data used for prediction matches the data used for model fitting in terms of size and structure. Handling missing values or ensuring the data consistency between the model fitting and augmentation steps will resolve this error.

You said:
ggplot(combined_data, aes(x = days_since_start)) +
  geom_line(aes(y = fitted_pct_dem, color = "DEM")) +
  geom_line(aes(y = fitted_pct_rep, color = "REP")) +
  scale_color_manual(values = c("DEM" = "blue", "REP" = "red"), 
                     name = "Party",
                     labels = c("DEM (Blue)", "REP (Red)")) +
  theme_classic() +
  labs(y = "Percent", x = "Days") +
  theme(legend.position = "top") how to display the predict result on final day
ChatGPT said:
ChatGPT
To display the predicted results on the final day in the plot, you can add geom_text() or geom_label() to annotate the values at the last day of days_since_start. Here’s how to modify your code:

r
Copy code
# Find the last day in the data
last_day_data <- combined_data %>%
  filter(days_since_start == max(days_since_start))

# Plot with annotation for the final day
ggplot(combined_data, aes(x = days_since_start)) +
  geom_line(aes(y = fitted_pct_dem, color = "DEM")) +
  geom_line(aes(y = fitted_pct_rep, color = "REP")) +
  geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = fitted_pct_dem,
      label = paste("DEM:", round(fitted_pct_dem, 1))
    ),
    color = "blue",
    vjust = -0.5
  ) +
  geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = fitted_pct_rep,
      label = paste("REP:", round(fitted_pct_rep, 1))
    ),
    color = "red",
    vjust = -0.5
  ) +
  scale_color_manual(values = c("DEM" = "blue", "REP" = "red"), 
                     name = "Party",
                     labels = c("DEM (Blue)", "REP (Red)")) +
  theme_classic() +
  labs(y = "Percent", x = "Days") +
  theme(legend.position = "top")
Explanation:
geom_text(): Adds text labels for the final day's predictions.
round(): Formats the predicted percentages to one decimal place.
vjust: Adjusts the vertical positioning of the labels to prevent overlap.

You said:
for combined data, there are data on the same day so it's vertical line for each day, how to take average of them and geta result
ChatGPT said:
ChatGPT
To calculate the average of values for each day in combined_data and get a single result per day, you can use group_by() and summarize() from the dplyr package:

r
Copy code
library(dplyr)

# Calculate the average for each day
combined_data_avg <- combined_data %>%
  group_by(days_since_start) %>%
  summarize(
    avg_fitted_pct_dem = mean(fitted_pct_dem, na.rm = TRUE),
    avg_fitted_pct_rep = mean(fitted_pct_rep, na.rm = TRUE)
  )

# Now, you can use this `combined_data_avg` in your plot
ggplot(combined_data_avg, aes(x = days_since_start)) +
  geom_line(aes(y = avg_fitted_pct_dem, color = "DEM")) +
  geom_line(aes(y = avg_fitted_pct_rep, color = "REP")) +
  scale_color_manual(values = c("DEM" = "blue", "REP" = "red"), 
                     name = "Party",
                     labels = c("DEM (Blue)", "REP (Red)")) +
  theme_classic() +
  labs(y = "Percent", x = "Days") +
  theme(legend.position = "top")
Explanation:
group_by(days_since_start) groups the data by days_since_start.
summarize() calculates the average for fitted_pct_dem and fitted_pct_rep for each day.
The na.rm = TRUE argument ensures that missing values are excluded from the calculation.

You said:
is there a way to only show the significant level of predictors in summary(model)
ChatGPT said:
ChatGPT
To show only the significant predictors from summary(model), you can use broom to tidy the output and filter based on p-values:

r
Copy code
library(broom)

# Get the tidy summary of the model
model_summary <- tidy(model)

# Filter for significant predictors (e.g., p-value < 0.05)
significant_predictors <- model_summary %>%
  filter(p.value < 0.05)

# Print the significant predictors
print(significant_predictors)
Explanation:
tidy(): Extracts the model summary into a data frame.
filter(p.value < 0.05): Keeps only rows where the p-value indicates significance.
You can adjust the threshold (e.g., < 0.01 or < 0.1) based on your desired significance level.

You said:
I have 2 fig I want them on same row, how to add cap for each one #| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheck
#| fig-cap: "Posterior prediction check for Bayesian models"
#| layout-ncol: 2


pp_check(bayesian_model_dem)
pp_check(bayesian_model_rep)
ChatGPT said:
ChatGPT
To add separate captions for each plot in the same row in Quarto (Qmd), you can specify fig-subcap for sub-captions. Here's how to modify your code:

markdown
Copy code
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheck
#| fig-cap: "Posterior prediction check for Bayesian models"
#| layout-ncol: 2
#| fig-subcap: ["Democratic Model", "Republican Model"]

pp_check(bayesian_model_dem)
pp_check(bayesian_model_rep)
Explanation:
fig-subcap: Specifies an array of sub-captions for each figure, matching their order.
layout-ncol: 2: Places the plots in two columns, side by side.
This setup will render the plots in the same row with individual sub-captions below each plot.