---
title: "Forecasting the 2024 U.S. Presidential Election Results: Linear Regression, Bayesian and Spline-Fit Gaussian Model by Using Pollsters' Data From Five Thirty Eight"
subtitle: "Contrasting Harris's Lead: Strong in Linear Regression and Bayesian Model, Modest in Spline-Fit Gaussian Model"
author: 
  - Ziqi Zhu
  - Yuanchen Miao
  - Claire Chang
thanks: "Code and data are available at: https://github.com/zzq20010617/2024_USelection_prediction"
date: today
date-format: long
abstract: "We utilize FiveThirtyEight’s 2024 U.S. presidential poll-of-polls data in 2024 to analyze support trends for the leading candidates, Kamala Harris and Donald Trump. By focusing on variables shows the reliability of a pollster, such as sample size, numeric scores, poll scores, and state-level factors, the data is fit into three models: a linear regression model, a Bayesian model, and a spline-fit Gaussian model. The results of the analysis indicated a strong lead for Harris in both the linear regression and Bayesian models, with a more modest lead in the spline-fit Gaussian model. This paper predicts the potential trajectory of the election, helping the public anticipate the policy directions that might follow a Harris administration."
number-sections: true
bibliography: references.bib
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(rstanarm)
library(splines)
library(broom)
library(knitr)
library(kableExtra)
```

```{r}
#| include: false
#| warning: false
#| message: false

cleaned_data <- read_csv(here::here("data/02-analysis_data/cleaned_data.csv"))
nationwide_data <- cleaned_data %>%  filter(state == "National")

# only two entries for Conservative Party
unique_parties <- unique(nationwide_data$party)

# drop columns that have NA in numeric_grade or pollscore, and 2 entries for CON party
nationwide_data <- nationwide_data %>%
  drop_na(numeric_grade, pollscore) %>%
  filter(party != "CON") %>%
  mutate(days_since_start = as.numeric(end_date - min(end_date)))

unique_parties <- unique(nationwide_data$party)
nationwide_data$party <- as.factor(nationwide_data$party)

nationwide_data <- nationwide_data %>% mutate(
  num_party = round((pct / 100) * sample_size, 0)
)
# filter nationwide_data by parties
dem_data <- nationwide_data %>%
  filter(party == "DEM")

dem_data <- dem_data |>
  mutate(
    pollster = factor(pollster)
  )

rep_data <- nationwide_data %>%
  filter(party == "REP")

rep_data <- rep_data |>
  mutate(
    pollster = factor(pollster)
  )
```

# Introduction

Nowadays, U.S. presidential elections have been marked by intense public interest and polarization. The race between Donald Trump and Kamala Harris, both of them are having a significant leads over other candidates, is currently the most closely watched contest. Polling data is now playing a very important role in showing the public perceptions and support rate to each candidates. Analyzing polling data with statistical models, we are able to have a statistical approach into candidate support rate across different states and parties. Analysis on polling data could help the public to predict the shifts in national, economic and social directions. Since the stances hold by Trump and Harris on issues like healthcare, immigration, and the international relations are in distinct positions, analyzing on polling data to predict the winner of the 2024 U.S. presidential election is very important to the public. This paper is contributing to this understanding using the latest polling data and statistical models to capture the trends in support rates of these two candidates.

In this paper, polling data of 2024 U.S. presidential election from FiveThirtyEight [@polls], focusing specifically on the support rate of the leading candidates, Kamala Harris and Donald Trump, is analyzed. The polling data from FiveThirtyEight [@polls] aggregates polls from various sources, thus, to ensure the reliability on the forecast result and analysis, we reduced the variables of the data to only retain variables that reflect polling reliability, such as sample size, numeric scores and poll scores. Additionally, the state-level data is kept as a factor influencing support rates, acknowledging the geographic variations in voter preferences.

The primary estimand in this study is the expected percentage of support (pct) for a U.S. presidential candidate, given polling data and relevant predictor variables like rating of the pollster and sample size of the poll.

The analysis employs three statistical models: linear regression model, Bayesian model and spline-fit Gaussian model. The findings indicates that Kamala Harris maintains a strong lead in both linear regression model and Bayesian model, while her leads becomes modest when comparing the trend of support rate of Donald Trump in spline-fit Gaussian model. Comparing to Donald Trump, Kamala Harris has a 2.6 percent lead in linear regression model, 23 more supporter out of 1200 in Bayesian model and only 1.06 percent lead in spline-fit Gaussian model.

These results are significant since they show the evolving dynamics of the election and how different statistical models and approaches could yield varying interpretations of candidate support rate. This paper is giving public an idea of who is the likely winner of the election that will have the potential implications on future political strategies and the overall direction of U.S. policy.

The remainder of this paper is structured as follows. @sec-data discusses the data used for this analysis, including key variables and sources, with particular attention to the quality metrics that affect polling accuracy. @sec-models outlines our modeling approach for each candidate, incorporating lessons learned from recent electoral cycles. @sec-predict presents our *** predictions based on the model outputs. @sec-discuss discusses the implications of our findings and suggests directions for future research. Finally, @sec-appendix evaluates Trafalgar Group’s polling methodology, and our idealized methodology and survey copy.

# Data {#sec-data}

## Overview

Our data is download from [Five Thirty Eight] (https://projects.fivethirtyeight.com/polls) [@polls], the website gathered survey data from different pollsters of 2024 US president election, We use the statistical programming **language R** [@citeR], and packages **lubridate** [@lubridate], **dplyr** [@dplyr], **tidyverse** [@tidyverse], **model summary** [@models], and **arrow** [@arrow] to process the data. Also, we use **tibble** [@tibble] to create the table for simulate table and save it as csv files. Graphs are created by using ggplot2 in from package **tidyverse** [@tidyverse], packages **broom** [@broom] and **knitr** [@knitr] were used to generate clean summary and table in this paper. Following **Telling stories with data** [@telling_stories_with_data], we create different models for the candidates from different parties in general state and forecasting the US president election in 2024.

## Cleaned Data 

The raw data obtained from FiveThirtyEight [@polls] initially contains 17,440 observations, which we reduced to 12,185 by only retaining polls conducted after January 1, 2024. As our analysis focuses specifically on Kamala Harris and Donald Trump, data collected before January 1, 2024, holds limited relevance due to the inclusion of outdated candidates. Reducing the data further to only include polls after Harris officially announced her candidacy would result in too few observations to support a robust model. Additionally, the "state" column in the dataset was adjusted by replacing all "NA" values with "National," as these "NA" entries represent polls conducted nationwide in the raw data. A view of the first five rows of cleaned data is shown in @tbl-cleandatarows

```{r}
#| echo: false
#| eval: true
#| label: tbl-cleandatarows
#| tbl-cap: "First Five Rows of Cleaned Data"
#| warning: false

cleaned_data_subset <- head(cleaned_data, 5)
colnames(cleaned_data_subset) <- c("pollster"	, "sample size", "poll grade", 
                                   "pollscore","state", "transparency score", 
                                   "poll end date", "party", "candidate", 
                                   "percentage")
kable(cleaned_data_subset, digits = 2) %>%
    kable_styling(font_size = 6) 
```


## Measurement

Polling data is collected by different pollsters using various survey methods reflect public opinion on the 2024 U.S. presidential election. Information of individuals collected by pollsters is being separate to different groups based on age, gender, race, occupation and geographic to reflect a more accurate supporting trend of each candidates involved in the presidential elections. From FiveThirtyEight, poll-of-polls dataset included the pollscore and numeric grade of the pollster which influence the reliability of the pollster on the collection of support rate of each candidates. Polling methodology is also various among pollsters, such as online panels, app panels, online Ad and phone interviews. Different methodology are having its own target audiences which the data collected can represent the opinion of specific age group.

## Outcome variables {#sec-octvar}

Our primary outcome variable is **support percentage (pct)**, which represents the percentage of respondents who support each candidate. This variable is modeled as the response variable in the MLR analyse.

## Predictor variables {#sec-prevar}

### Sample Size

Sample size is an important predictor because it influences the reliability of a poll. Larger sample sizes tend to reduce sampling error, providing more accurate reflections of voter sentiment. In our MLR model, the sample size is log-transformed to account for diminishing returns—larger polls do not necessarily offer proportionally better accuracy.

### Poll Score

The poll score is a measure of the error and bias we can attribute to a pollster, negative number is better.

### Numeric Grade

This variable is an aggregate score of the poll based on a numeric scale. It serves as an indicator of the pollster's historical performance and the methodology used. 3 is the maximum and some pollsters have no rating.

### Transparency Score

Transparency score measures how openly a pollster reports their methodology and results. A higher transparency score suggests that the pollster has disclosed key details about how the poll was conducted, improving trust in the poll results.

### Days Since Start

This variable represents the number of days since the beginning of the election polling period. It helps capture the dynamic nature of voter preferences over time, accounting for shifts in public opinion as the election date nears.

# Models {#sec-models}

The goal of our modelling strategy is try to capture the trend of support percentage for different parties in 2024 Presidential election as much as possible. In the following section we briefly describe the models we used to investigate. Background details and diagnostics are included in [Appendix -@sec-model-details].

## MLR Model set-up

We first filtered our data by different parties, as the single linear model but include other 3 parties(GRE, IND, and LIB, Conservative only has two entries in cleaned nationwide data and has 0 pct so its been removed), and fit linear model for each of them. The formula for Multiple linear regression is as follow \begin{align}
\text{pct}_i &= \beta_0 + \beta_1 \log(\text{sample\_size}_i) + \beta_2 \, \text{pollscore}_i + \beta_3 \, \text{numeric\_grade}_i \\
&+ \beta_4 \, \text{transparency\_score}_i + \beta_5 \, \text{days\_since\_start}_i + \epsilon_i
\end{align}

Response Variable is pct. Predictors are log(sample_size), pollscore, numeric_grade, transparency_score, and days_since_start, detail can be find in [@sec-octvar], and [@sec-prevar]

### Result

Our results for the MLR model are summarized in @tbl-mlrmodelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

MLR_models <-
  readRDS(file = here::here("models/MLRmodels.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-mlrmodelresults
#| tbl-cap: "Explanatory models of pct based on pollscore, sample size, and time"
#| warning: false

modelsummary::modelsummary(
  MLR_models,
  fmt = 2
)
```

First, we checked the residual vs. fitted plot as shown in [Appendix @sec-model-details] and saw no obvious violation of assumptions. According to the summary, DEM and REP models are based on the largest number of observations (both 1235). Smaller parties have fewer observations, which may lead to less robust models. The $r^2$ shows that the model explains the highest amount of variance on the Democratic Party than others with a value of 0.357, and other parties less than 0.2. Based on the summary of models for DEM and REP [@sec-model-sigcheck], we are not sure if those predictors that measure the quality of polls are significant, so we keep them for now in the Bayesian model. The prediction of MLR models is shown in [@fig-mlrpredict], in which we can see the blue line represents the Democratic party has a steeper slope and starts to take the lead between day 100 and day 200, and the average support percentage on the last day of data shows that Democratic (Harris) has a slight advantage of 2.6 percent.

```{r}
#| echo: false
#| label: fig-mlrpredict
#| fig-cap: "Prediction with MLR models for DEM and REP party"
#| warning: false

# Augment the datasets with predictions
dem_data <- dem_data |>
  mutate(fitted_pct_dem = predict(MLR_models[['DEM']]))
rep_data <- rep_data |>
  mutate(fitted_pct_rep = predict(MLR_models[['REP']]))

# Combine both datasets into one
combined_data <- bind_rows(dem_data, rep_data)

# Take average of pct for each day
combined_data_avg <- combined_data %>%
  group_by(days_since_start) %>%
  summarize(
    avg_fitted_pct_dem = mean(fitted_pct_dem, na.rm = TRUE),
    avg_fitted_pct_rep = mean(fitted_pct_rep, na.rm = TRUE)
  )

# Extract the last day data for annotations
last_day_data <- combined_data_avg %>%
  filter(days_since_start == max(days_since_start))

# Plot with colors based on the party
ggplot(combined_data_avg, aes(x = days_since_start)) +
  geom_line(aes(y = avg_fitted_pct_dem, color = "DEM")) +
  geom_line(aes(y = avg_fitted_pct_rep, color = "REP")) +
  scale_color_manual(values = c("DEM" = "blue", "REP" = "red"), 
                     name = "Party",
                     labels = c("DEM (Blue)", "REP (Red)")) +
  geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = avg_fitted_pct_dem,
      label = paste("DEM:", round(avg_fitted_pct_dem, 1))
    ),
    color = "blue",
    vjust = -0.5,
    hjust = 1
  ) +
  geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = avg_fitted_pct_rep,
      label = paste("REP:", round(avg_fitted_pct_rep, 1))
    ),
    color = "red",
    vjust = -0.5,
    hjust = 1
  ) +
  theme_classic() +
  labs(y = "Percent", x = "Days") +
  theme(legend.position = "top")
```

## Bayesian Model set-up

The Bayesian model is built from the MLR models from the above section. With inspiration from the example R code, we introduce the random effects for Pollsters, and we use a logistic link to predict the probability of support, modeling count data which is in a binomial distribution. We choose to use a default prior and enable autoscale, the formula is shown below.

```{=tex}
\begin{align} 
\log \left( \frac{p_i}{1 - p_i} \right) &= \beta_0 + \beta_1 \log(\text{sample\_size}_i) + \beta_2 \, \text{pollscore}_i + \beta_3 \, \text{numeric\_grade}_i \\
&+ \beta_4 \, \text{transparency\_score}_i + \beta_5 \, \text{days\_since\_start}_i + \alpha_j\\
\alpha_j &\sim \text{Normal}(0, \sigma_{\text{pollster}}) \\
\beta_0 &\sim \text{Normal}(0, 2.5) \\
\beta_1 &\sim \text{Normal}(0, 2.5) \\
\beta_2 &\sim \text{Normal}(0, 2.5) \\
\beta_3 &\sim \text{Normal}(0, 2.5) \\
\beta_4 &\sim \text{Normal}(0, 2.5) \\
\beta_5 &\sim \text{Normal}(0, 2.5) \\
\sigma_{\text{pollster}} &\sim \text{Exponential}(1)
\end{align}
```
$y_i$ represents the number of individuals in the sample that support the Democratic party (this corresponds to num_party). The response is modeled as binomial: $y_i \sim \text{Binomial}(\text{sample\_size}_i, p_i)$, where $p_i$ is the probability that an individual in poll $i$ supports the Democratic party, and $\text{sample\_size}_i$ is the total number of individuals surveyed in poll $i$. We also create a model for the Republican party to compare the trend in prediction results in the following section [@sec-bayresult].

### Result {#sec-bayresult}
To get a prediction by the Bayesian model, we create a data frame and generate posterior predictions by the new data frame, this table [@tbl-newdata] shows the first several rows, and we set the quality scale of predicted data to the best to perform a poll that has high quality. A summary of the Bayesian model is shown in [@tbl-baymodelresults], and the posterior predictive check is in appendix [@sec-model-ppcheck]. The predicted result is shown in [@fig-baypredict].

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

bayesian_model_dem <-
  readRDS(file = here::here("models/bayesian_model_dem.rds"))
bayesian_model_rep <-
  readRDS(file = here::here("models/bayesian_model_rep.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-baymodelresults
#| tbl-cap: ""
#| warning: false

modelsummary::modelsummary(
  bayesian_model_dem,
  fmt = 2
)
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: tbl-newdata

# Create new data for prediction
new_data <- data.frame(
  num_party = 0,
  pollscore = -1.5,
  sample_size = 1200,
  numeric_grade = 3,
  transparency_score=10,
  days_since_start = seq(
    min(dem_data$days_since_start),
    max(dem_data$days_since_start),
    length.out = 100
  ),
  pollster = factor("YouGov", levels = levels(dem_data$pollster))
)

head(new_data, 3)
```

```{r}
#| echo: false
#| warning: false
#| message: false

set.seed(123) # set seed for reproduce of predict result
predict_counts_dem <- posterior_predict(bayesian_model_dem, newdata = new_data)
predict_counts_rep <- posterior_predict(bayesian_model_rep, newdata = new_data)

combined_data <- bind_rows(dem_data, rep_data)
pred_summary_dem <- new_data %>%
  mutate(
    pred_mean_dem = colMeans(predict_counts_dem),
    pred_lower_dem = apply(predict_counts_dem, 2, quantile, probs = 0.025),
    pred_upper_dem = apply(predict_counts_dem, 2, quantile, probs = 0.975)
  )

# Summarize predictions for REP
pred_summary_rep <- new_data %>%
  mutate(
    pred_mean_rep = colMeans(predict_counts_rep),
    pred_lower_rep = apply(predict_counts_rep, 2, quantile, probs = 0.025),
    pred_upper_rep = apply(predict_counts_rep, 2, quantile, probs = 0.975)
  )

# Combine both summaries
combined_summary <- pred_summary_dem %>%
  select(days_since_start, pred_mean_dem, pred_lower_dem, pred_upper_dem) %>%
  left_join(
    pred_summary_rep %>% 
      select(days_since_start, pred_mean_rep, pred_lower_rep, pred_upper_rep),
    by = "days_since_start"
  )
# Extract the last day data for annotations
last_day_data <- combined_summary %>%
  filter(days_since_start == max(days_since_start))
```

```{r}
#| echo: false
#| label: fig-baypredict
#| fig-cap: "Predicted number of supporters for DEM and REP party"
#| warning: false

ggplot(combined_summary, aes(x = days_since_start)) +
  geom_line(aes(y = pred_mean_dem, color = "DEM"), size = 1) +
  geom_ribbon(aes(ymin = pred_lower_dem, ymax = pred_upper_dem, fill = "DEM"), alpha = 0.2) +
  geom_line(aes(y = pred_mean_rep, color = "REP"), size = 1) +
  geom_ribbon(aes(ymin = pred_lower_rep, ymax = pred_upper_rep, fill = "REP"), alpha = 0.2) +
  scale_color_manual(values = c("DEM" = "blue", "REP" = "red")) +
  scale_fill_manual(values = c("DEM" = "blue", "REP" = "red")) +
  geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = pred_mean_dem,
      label = paste("DEM:", round(pred_mean_dem))
    ),
    color = "blue",
    vjust = -1,
    hjust = 1
  ) +
  geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = pred_mean_rep,
      label = paste("REP:", round(pred_mean_rep))
    ),
    color = "red",
    vjust = -1,
    hjust = 1
  ) +
  labs(
    x = "Days Since Start",
    y = "Predicted Number of Supporters",
    color = "Party",
    fill = "Party"
  ) +
  theme_minimal()
```

The prediction results of the Bayesian model closely align with those of the linear model, with both showing the mean predicted number of supporters for each party. Notably, the Democratic Party takes the lead around day 210 (late July). By early November, the predictions indicate that the Democratic Party has an average of 601 supporters, compared to the Republican Party's 578. This suggests a slight advantage for the Democrats as the final prediction period concludes.

## Spline Fit model set-up

We first run the model in R [@citeR] using the `rstanarm` package of [@rstanarm], and use priors $\text{Normal}(0, 5)$ to allow for more flexibility of predictors effects with “autoscale = TRUE”. Then we use the same data frame that was used to predict for the Bayesian model [@tbl-newdata], the result is shown as [@sec-splresult]

### Result {#sec-splresult}

```{r}
#| echo: false
#| warning: false
#| message: false

spline_dem <-
  readRDS(file = here::here("models/spline_dem.rds"))
spline_rep <-
  readRDS(file = here::here("models/spline_rep.rds"))
```

```{r}
#| echo: false
#| warning: false
#| message: false

posterior_preds_dem <- posterior_predict(spline_dem, newdata = new_data)
posterior_preds_rep <- posterior_predict(spline_rep, newdata = new_data)

combined_data <- bind_rows(dem_data, rep_data)

# Summarize predictions
pred_summary_dem <- new_data |>
  mutate(
    pred_mean_dem = colMeans(posterior_preds_dem),
    pred_lower_dem = apply(posterior_preds_dem, 2, quantile, probs = 0.025),
    pred_upper_dem = apply(posterior_preds_dem, 2, quantile, probs = 0.975),
    party = "DEM"
  )

# Summarize predictions for REP
pred_summary_rep <- new_data |>
  mutate(
    pred_mean_rep = colMeans(posterior_preds_rep),
    pred_lower_rep = apply(posterior_preds_rep, 2, quantile, probs = 0.025),
    pred_upper_rep = apply(posterior_preds_rep, 2, quantile, probs = 0.975),
    party = "REP"
  )

# Combine both summaries into a single dataframe
pred_summary <- bind_rows(pred_summary_dem, pred_summary_rep)

# Extract the last day data for annotations
last_day_data <- pred_summary %>%
  filter(days_since_start == max(days_since_start))
```

```{r}
#| echo: false
#| label: fig-splpredict
#| fig-cap: "Predicted support percentage over Time with Spline Fit for DEM and REP party"
#| warning: false

ggplot(pred_summary, aes(x = days_since_start)) +
  geom_line(aes(y = pred_mean_dem, color = "DEM"), size = 1, data = subset(pred_summary, party == "DEM")) +
  geom_ribbon(aes(ymin = pred_lower_dem, ymax = pred_upper_dem, fill = "DEM"), alpha = 0.2, data = subset(pred_summary, party == "DEM")) +
  geom_line(aes(y = pred_mean_rep, color = "REP"), size = 1, data = subset(pred_summary, party == "REP")) +
  geom_ribbon(aes(ymin = pred_lower_rep, ymax = pred_upper_rep, fill = "REP"), alpha = 0.2, data = subset(pred_summary, party == "REP")) +
  scale_color_manual(values = c("DEM" = "blue", "REP" = "red")) +
  scale_fill_manual(values = c("DEM" = "blue", "REP" = "red")) +
    geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = pred_mean_dem,
      label = paste("DEM:", round(pred_mean_dem, 2))
    ),
    color = "blue",
    vjust = -1,
    hjust = 1
  ) +
  geom_text(
    data = last_day_data,
    aes(
      x = days_since_start,
      y = pred_mean_rep,
      label = paste("REP:", round(pred_mean_rep, 2))
    ),
    color = "red",
    vjust = -1,
    hjust = 1
  ) +
  labs(
    x = "Days Since Start",
    y = "Predicted Number of Supporters",
    color = "Party",
    fill = "Party"
  ) +
  theme_minimal()
```

The prediction graph [@fig-splpredict] contains a prediction of the spline fit model for two parties, the blue line represents the DEM party's predicted percentage of support, and the red line represents the REP party's predicted percentage of support over time (measured in days since the start of 2024). We can tell that the predicted support of the DEM party is taking the lead by around 2 percent until the last day of the dataset.

# Discussion {#sec-discuss}

## Crossing point in the prediction {#sec-first-point}

From the prediction of spline fit model [@fig-splpredict], There is a clear increase in the support for the DEM party starting around day 200, while the REP party's support remains more stable with only a slight increase over time. The increase in support for the DEM party in the predictions could be attributed to Harris's rise as the Democratic candidate after Biden's exit from the race On July 21, 2024, which is about 230 days from start of 2024. By analyzing the support percentages of Biden and Harris from the Democratic poll data @fig-demonly, it becomes evident that their combined impact likely contributes to the double peak seen in the posterior predictive check @fig-ppcheck. This dual pattern challenges the model’s ability to capture trends accurately

```{r}
#| echo: false
#| label: fig-demonly
#| fig-cap: "Density Plot of Support Percentage for Biden and Harris in 2024"
#| warning: false

dem_data_filtered <- dem_data %>%
  filter(answer %in% c("Biden", "Harris"))

# Create the density plot
ggplot(dem_data_filtered, aes(x = pct, fill = answer)) +
  geom_density(alpha = 0.4) +
  labs(
    x = "Support Percentage",
    y = "Density",
    fill = "Candidate"
  ) +
  theme_minimal()
```

## Differences in Modeling Nationwide Polls and Actual Election Results

In this paper, we used nationwide poll data for modeling, which contrasts significantly with using state-by-state election results. Nationwide polls offer general insights into aggregate support levels but do not account for the Electoral College system in U.S. presidential elections. This approach can overlook state-specific variations and the unique influence of each state's electoral votes, leading to potential bias where larger states disproportionately affect overall results. Conversely, state-level modeling captures the distribution of support across states, highlighting the impact of battleground states. To enhance prediction accuracy and align with the actual election structure, future models should integrate state-level data especially for the battleground states for more robust, applicable results.

## Weaknesses and next steps

Firstly we focused only on nationwide polls, excluding state-specific data, due to limited and outdated information from certain states. For example, Mississippi had only four polls, with the most recent conducted in April 2024, making state-level modeling and vote counting challenging. This limitation restricts the accuracy of state-by-state analysis. With more reliable and updated data, using a state-based approach would enhance the model’s precision and provide a more comprehensive prediction of the election outcome, especially for discovering the public opinion in battleground states.
Furthermore modeling Biden and Harris together under one Democratic model introduces potential inaccuracies, especially influencing the slope of predictions in both linear and Bayesian models, making the observed upward trend less reliable.

\newpage

\appendix

# Appendix {#sec-appendix}

## Idealized Methodology

To forecast the 2024 U.S. presidential election with a budget of $100,000, this methodology combines stratified sampling, multimodal recruitment, and aggregation techniques that leverage the strengths of multiple data sources. The approach employs stratified random sampling with targeted oversampling of key subgroups, like younger and minority voters, to address their frequent underrepresentation in polling. By segmenting eligible U.S. voters by demographics—age, gender, race, party affiliation, region, and urban/rural residency—the method ensures broad representational coverage, aiming to capture diverse voting patterns accurately [@pewpolling]. Partnering with a voter database provider gives us access to a detailed list of registered voters, organized by demographics, to improve the accuracy of our sample. This approach also builds on public trust, as Americans generally trust polls from news organizations (43%) more than those from websites that combine multiple polls (30%) [@PublicOpinionQuarterly].

Respondents are recruited through multiple channels to reduce potential biases associated with any single mode. The multimodal approach includes Interactive Voice Response (IVR) to reach older and rural populations (30%), targeted online surveys to capture difficult-to-reach demographics (40%), text surveys directed at younger voters (20%), and live calls in key battleground states (10%). This mix helps address “coverage error” by compensating for the limitations associated with each mode [@Blumenthal_2014].

Survey data is collected via a short, 5–7 minute questionnaire with 10 core questions covering demographics, voting intention, likelihood of voting, and key issues. A shorter survey length and mix of direct and indirect questions help reduce Social Desirability Bias, while randomizing question order minimizes positional bias. Given that any individual survey is likely to suffer from random and systematic errors—sampling errors, coverage errors, and response biases—aggregation across multiple polls can mitigate these sources of error, improving the accuracy of forecasts by averaging out errors specific to individual polls [@Blumenthal_2014].

For data validation and quality control, screening questions confirm attentiveness, demographic cross-checks validate responses against voter registration data, and post-survey weighting adjusts for demographic imbalances. Weighting responses to match population demographics ensures that key groups are proportionally represented, while data cleaning removes low-quality responses to maintain data integrity [@Blumenthal_2014]. To further address biases, surveys are conducted anonymously in formats like online and SMS. Weighting adjustments are also made to address non-response bias, enhancing accuracy by reflecting the true population distribution. We will keep the process and result transparent by posting on our website for the public to view. 

Weekly aggregation serves as a “poll of polls” that balances fluctuations across samples and smooths out errors. By incorporating recent data more heavily, this rolling average adapts to shifts in voter sentiment leading up to the election. Bayesian modeling, applied to smooth sample variations and incorporate prior election trends, stabilizes the forecast further. Poll aggregation, which is commonly employed in election analyses, effectively combines estimates from various samples, reducing random error and discounting non-universal biases [@Blumenthal_2014]. Given the variability in survey methodologies—such as IVR surveys missing cell-only populations or web surveys excluding offline individuals—aggregation allows errors in one type of survey to offset those in another. Weighting each survey according to sample size and precision further refines the accuracy of the aggregated forecast.


### Budget Allocation:

- **Sample Acquisition**: Partnership with a voter database provider to access a list of registered voters across all states $10,000

- **Interactive Voice Response (IVR)**: Automated voice surveys for older and rural demographics $20,000

- **Online Surveys**: Targeted ads and opt-in digital surveys to capture harder-to-reach groups (40% of sample) $25,000

- **Text Messaging**: SMS-based surveys to engage younger demographics (20% of sample) $10,000

- **Live Phone Calls**: Calls in battleground states to reach underrepresented groups (10% of sample) $20,000

- **Data Validation & Cleaning**:	Screening questions, demographic verification, and removal of low-quality responses to maintain data integrity $5,000

- **Data Analysis & Forecast Modeling**	Bayesian modeling, poll aggregation, and analysis of survey trends to produce election forecasts $10,000


## Idealized Survey

The proposed survey questionnarie design is in the following link: https://forms.gle/Zm5Kfj3kL58gwCz38

### Survey Copy

1. **What is your age??**
   - 18-29
   - 30-44
   - 45-64
   - 65+
   
2. **What is your gender identity?**
   - Female
   - Male
   - Another Gender Identity 
  
3. **Which of the following best describes your race/ethnicity?**
   - White
   - Black/African American
   - Hispanic/Latino
   - Asian
   - Native American
   - Prefer not to say
   - Other

4. **What is your highest level of education?**
   - High school or less
   - College
   - Bachelor's degree
   - Graduate degree
   - Prefer not to say
  
5. **In which U.S. region do you currently reside?**
   - Midwest
   - Northeast
   - South
   - West
   
6. **Are you registered to vote in the 2024 presidential election?**
   - Yes
   - No
  
7. **2024 Which candidate do you plan to vote for?**
   - Kamala Harris
   - Donald Trump
   - Undecided
   - Other

8. **2020 Which candidate did you vote for?**
   - Joe Biden
   - Donald Trump
   - Did not vote
   - Prefer not to say
   - Other
  
9. **What is the most important issue influencing your vote?**
   - Economy
   - Healthcare 
   - Immigration
   - Climate Change
   - Social Justice
   - National Security
   - Other

10. **On a scale of 1-5, how likely are you to vote in the 2024 U.S. presidential election?**

   - 1 being Not Likely to Vote
   - 5 being Likely to Vote


## Trafalgar group’s methodology overview and evaluation

The Trafalgar Group conducts polls ranging from major political campaigns to marketing surveys. The organization's 0.7 pollster rating indicates moderate accuracy and reliability compared to other pollsters. The population consists of all eligible voters in the U.S., while the sampling frame includes registered voters across states, segmented by demographics like age, gender, party affiliation, and ethnicity. Trafalgar Group typically samples likely voters,focusing on those most likely to participate in upcoming elections. Trafalgar Group recruits its sample using a mix of interactive voice response, live phone calls, text messages, emails, digital dial back interface and online targeted opt-in digital survey platforms [@trafalgar_polling_methodology]. Trafalgar places particular emphasis on reducing Social Desirability Bias, designing short questionnaires and using nontraditional question formats to help participants feel more comfortable expressing their true preferences, especially on sensitive topics [@trafalgar_polling_methodology]. They also adjust for non-response by weighting the sample, ensuring it reflects the broader population’s demographic composition. While this helps to reduce bias from underrepresented groups, reliance on post-survey adjustments can introduce new biases, especially if response rates vary significantly among demographic groups.

# Model details {#sec-model-details}

## Assumption check for MLR models

```{r}
#| echo: false
#| eval: true
#| warning: false
#| fig.cap: "Residuals vs Fitted for DEM Party"

plot(MLR_models[['DEM']], which = 1)
```

```{r}
#| echo: false
#| eval: true
#| warning: false
#| fig.cap: "Residuals vs Fitted for REP Party"

plot(MLR_models[['REP']], which = 1)
```

## Significant check for MLR models {#sec-model-sigcheck}

see [@tbl-mlrmodelsum-dem], and [@tbl-mlrmodelsum-rep]

```{r}
#| echo: false
#| eval: true
#| label: tbl-mlrmodelsum-dem
#| tbl-cap: "Summary of DEM data fit by MLR model"
#| warning: false

model_summary <- tidy(MLR_models[['DEM']])
kable(model_summary)
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-mlrmodelsum-rep
#| tbl-cap: "Summary of REP data fit by MLR model"
#| warning: false

model_summary <- tidy(MLR_models[['REP ']])
kable(model_summary)
```

\newpage

## Posterior predictive check {#sec-model-ppcheck}

In @fig-ppcheck we implement a posterior predictive check. The check for DEM model shows the observed data has two peaks, indicating a possible bimodal distribution, but the predictive simulations also show significant variability around the peaks. While the model for REP performs better, capturing the main distribution of the data

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheck
#| fig-cap: "Posterior prediction check for Bayesian models"
#| layout-ncol: 2
#| fig-subcap: ["Democratic Model", "Republican Model"]

pp_check(bayesian_model_dem) +
  theme_classic() +
  theme(legend.position = "bottom")
pp_check(bayesian_model_rep) +
  theme_classic() +
  theme(legend.position = "bottom")

```

\newpage

# References
