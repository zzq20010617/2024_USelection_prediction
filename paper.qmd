---
title: "Forecasting the US Presidential Election: A Poll-of-Polls Approach Using Linear Models"
author: 
  - Ziqi Zhu
  - Yuanchen Miao
  - Claire Chang
thanks: "Code and data are available at: https://github.com/zzq20010617/2024_USelection_prediction"
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
number-sections: true
bibliography: references.bib
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
```


# Introduction

This paper examines the development of a multiple linear regression (MLR) model to predict the percentage of support (pct) for U.S. presidential candidates based on polling data. The data includes a range of predictors, such as sample size, pollster quality factors, and time-related factors. The focus of the analysis is on nationwide polling data, aggregated from different pollsters, to create a robust model for forecasting election outcomes. The goal is to provide a clearer understanding of how various poll attributes influence polling results and to derive insights that can predict election outcomes.

The primary estimand in this study is the expected percentage of support (pct) for a U.S. presidential candidate, given polling data and relevant predictor variables like rating of the pollster and sample size of the poll.

Our Results shows that

This analysis is useful in understanding how various polling factors contribute to predicting election results. By modeling the relationship between polling result and factors, the study enhances the ability to forecast election outcomes based on public opinion data. This model provides a practical tool for researchers, political strategists, and analysts to assess the reliability of polling data and its implications for elections.


# Data {#sec-data}

## Overview

Our data is download from [@polls], the website gathered survey data from different pollsters, We use the statistical programming language R [@citeR], and packages [@lubridate], [@dplyr], [@tidyverse], to process the data. Following @tellingstories, we consider...

## Measurement
	
The dataset is about public opinions about U.S. presidential candidates, which are captured through polling.

## Outcome variables

Our primary outcome variable is **support percentage (pct)**, which represents the percentage of respondents who support each candidate. This variable is modeled as the response variable in the MLR analyses.

```{r}
#| label: fig-bills
#| fig-cap: Bills of penguins
#| echo: false

# ggplot(penguins, aes(x = island, fill = species)) +
#   geom_bar(alpha = 0.8) +
#   scale_fill_manual(values = c("darkorange","purple","cyan4"),
#                     guide = "none") +
#   theme_minimal() +
#   facet_wrap(~species, ncol = 1) +
#   coord_flip()
```

Talk more about it.

And also planes (@fig-planes). (You can change the height and width, but don't worry about doing that until you have finished every other aspect of the paper - Quarto will try to make it look nice and the defaults usually work well once you have enough text.)

```{r}
#| label: fig-planes
#| fig-cap: Relationship between wing length and width
#| echo: false
#| warning: false
#| message: false

# analysis_data <- read_csv(here::here("data/02-analysis_data/analysis_data.csv"))
# 
# analysis_data |> 
#   ggplot(aes(x = width, y = length)) +
#   geom_point(alpha = 0.8) +
#   theme_minimal() +
#   labs(x = "Wing width (mm)",
#        y = "Wing length (mm)")
```

Talk way more about it. 

## Predictor variables

### Sample Size

Sample size is an important predictor because it influences the reliability of a poll. Larger sample sizes tend to reduce sampling error, providing more accurate reflections of voter sentiment. In our MLR model, the sample size is log-transformed to account for diminishing returns—larger polls do not necessarily offer proportionally better accuracy.

### Poll Score

The poll score is a measure of the error and bias we can attribute to a pollster, negative number is better.

### Numeric Grade

This variable is an aggregate score of the poll based on a numeric scale. It serves as an indicator of the pollster's historical performance and the methodology used. 3 is the maximum and some pollsters have no rating.

### Transparency Score

Transparency score measures how openly a pollster reports their methodology and results. A higher transparency score suggests that the pollster has disclosed key details about how the poll was conducted, improving trust in the poll results.

### Days Since Start

This variable represents the number of days since the beginning of the election polling period. It helps capture the dynamic nature of voter preferences over time, accounting for shifts in public opinion as the election date nears.


# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results

Our results are summarized in @tbl-modelresults.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(rstanarm)

# first_model <-
#   readRDS(file = here::here("models/first_model.rds"))
```

```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

# modelsummary::modelsummary(
#   list(
#     "First model" = first_model
#   ),
#   statistic = "mad",
#   fmt = 2
# )
```




# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}


# Additional data details

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

# pp_check(first_model) +
#   theme_classic() +
#   theme(legend.position = "bottom")
# 
# posterior_vs_prior(first_model) +
#   theme_minimal() +
#   scale_color_brewer(palette = "Set1") +
#   theme(legend.position = "bottom") +
#   coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

# plot(first_model, "trace")
# 
# plot(first_model, "rhat")
```

## Trafalgar Group’s Methodology

The Trafalgar Group conducts polls ranging from major political campaigns to marketing surveys.  The organization's 0.7 pollster rating indicates moderate accuracy and reliability compared to other pollsters. The population consists of all eligible voters in the U.S., while the sampling frame likely includes registered voters across states, segmented by demographics like age, gender, and party affiliation. Trafalgar Group typically samples likely voters, focusing on those most likely to participate in upcoming elections. Trafalgar Group recruits its sample using a mix of interactive voice response (IVR), live phone calls, text messages, online panels, and email surveys. Trafalgar Group employs stratified random sampling to ensure proportional representation of subgroups like political party, gender, and region. This method improves accuracy by reflecting the electorate's demographic and political makeup but may risk over-stratification, giving smaller voter groups disproportionate influence and potentially skewing results. Trafalgar Group handles non-response by using weighting to adjust the sample, ensuring respondent demographics (age, race, education) match population parameters. This method reduces bias by accounting for underrepresented groups, though heavy reliance on post-survey adjustments may introduce new biases, particularly when there are large discrepancies in response rates across demographics.



\newpage


# References


